 #Environment setup -static obstacles


 1. Static Environment Setup-Fully Centralized TD3


class StaticEnvironment:
    def __init__(self, num_agents, state_dim, action_dim, obstacle_positions, goal_positions):
        self.num_agents = num_agents
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.obstacle_positions = obstacle_positions
        self.goal_positions = goal_positions
        self.agent_states = np.random.uniform(low=0, high=10, size=(num_agents, state_dim))

    def reset(self):
        self.agent_states = np.random.uniform(low=0, high=10, size=(self.num_agents, self.state_dim))
        return self.agent_states

    def step(self, actions):
        rewards = []
        new_states = []
        dones = []

        for i, action in enumerate(actions):
            state = self.agent_states[i]

            # Update state based on action
            new_state = state + action  # Simple dynamics: new position = current + action
            self.agent_states[i] = new_state

            # Reward: Encourage movement toward goal, penalize collision
            distance_to_goal = np.linalg.norm(new_state - self.goal_positions[i])
            collision = any(np.linalg.norm(new_state - obs) < 1.0 for obs in self.obstacle_positions)

            reward = -distance_to_goal
            if collision:
                reward -= 100  # Heavy penalty for collision

            rewards.append(reward)
            new_states.append(new_state)
            dones.append(distance_to_goal < 1.0 or collision)

        return np.array(new_states), np.array(rewards), np.array(dones)

    def render(self):
        pass  # Add visualization if needed


 2.For  Centralized TD3 Implementation


# TD3 Agent for Static Environment
class StaticTD3Agent(TD3Agent):
    def __init__(self, state_dim, action_dim, num_agents):
        super().__init__(state_dim * num_agents, action_dim * num_agents)  # Centralized state and action spaces
        self.num_agents = num_agents

    def select_action(self, states):
        """
        States include the concatenated states of all agents.
        """
        centralized_state = np.concatenate(states)
        centralized_action = super().select_action(centralized_state)
        return np.split(centralized_action, self.num_agents)  # Split actions for each agent


3.Workflow Integration
   1.Initialize the Environment and Agent:
# Define static environment
num_agents = 3
state_dim = 4  # Example: [x, y, vx, vy]
action_dim = 2  # Example: [vx, vy]
obstacle_positions = [np.array([5, 5]), np.array([8, 2])]
goal_positions = [np.array([10, 10]), np.array([10, 0]), np.array([0, 10])]

env = StaticEnvironment(num_agents, state_dim, action_dim, obstacle_positions, goal_positions)

# Initialize TD3 Agent
agent = StaticTD3Agent(state_dim, action_dim, num_agents)


2.Training Loop:
 # Training Loop
num_episodes = 500
replay_buffer = ReplayBuffer()

for episode in range(num_episodes):
    states = env.reset()
    episode_reward = 0

    for step in range(200):  # Max steps per episode
        actions = agent.select_action(states)
        next_states, rewards, dones = env.step(actions)

        # Store experience in replay buffer
        replay_buffer.store(np.concatenate(states), np.concatenate(actions), 
                            np.mean(rewards), np.concatenate(next_states), 
                            np.any(dones))

        # Update the TD3 agent
        agent.update(replay_buffer)

        states = next_states
        episode_reward += np.mean(rewards)

        if np.any(dones):
            break

    print(f"Episode {episode + 1}, Reward: {episode_reward}")















                        
