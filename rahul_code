#Step 3: Environment Setup

import numpy as np

class CustomUAVEnvironment:
    def __init__(self):
        # Environment parameters
        self.goal = np.array([10.0, 10.0])  # Target position
        self.state = None
        self.obstacles = [np.array([5.0, 5.0]), np.array([7.0, 3.0])]  # Static obstacle positions
        self.bounds = [0.0, 15.0]  # Environment bounds (x and y limits)
        
        # Initialize UAV dynamics
        self.max_velocity = 2.0
        self.time_step = 0.1  # Simulation time step

    def reset(self):
        """Reset the environment to the initial state."""
        self.state = np.array([0.0, 0.0, 0.0, 0.0])  # [x, y, velocity_x, velocity_y]
        return self.state

    def step(self, action):
        """Perform one step in the environment."""
        x, y, vx, vy = self.state
        
        # Action interpretation: Change in velocity
        if action == 0:  # Accelerate left
            vx -= 0.1
        elif action == 1:  # Accelerate right
            vx += 0.1
        elif action == 2:  # Accelerate up
            vy += 0.1
        elif action == 3:  # Accelerate down
            vy -= 0.1
        
        # Limit velocity to max_velocity
        vx = np.clip(vx, -self.max_velocity, self.max_velocity)
        vy = np.clip(vy, -self.max_velocity, self.max_velocity)
        
        # Update position
        x += vx * self.time_step
        y += vy * self.time_step
        
        # Ensure the UAV stays within bounds
        x = np.clip(x, self.bounds[0], self.bounds[1])
        y = np.clip(y, self.bounds[0], self.bounds[1])
        
        # Update the state
        self.state = np.array([x, y, vx, vy])
        
        # Check for collisions or goal reach
        collision = any(np.linalg.norm(self.state[:2] - obs) < 1.0 for obs in self.obstacles)
        goal_reached = np.linalg.norm(self.state[:2] - self.goal) < 1.0
        
        # Calculate reward
        reward = self._calculate_reward(collision, goal_reached)

        # Check if the episode ends
        done = collision or goal_reached
        return self.state, reward, done

    def _calculate_reward(self, collision, goal_reached):
        """Calculate the reward for the current step."""
        if goal_reached:
            return 100.0  # Large reward for reaching the goal
        if collision:
            return -50.0  # Penalty for collision
        return -0.1  # Small penalty for each step to encourage efficiency

    def render(self):
        """Visualize the environment (optional)."""
        print(f"UAV position: {self.state[:2]} | Goal: {self.goal}")

# Example usage
env = CustomUAVEnvironment()
state = env.reset()
done = False

while not done:
    action = np.random.choice(4)  # Random action for demonstration (replace with agent's action)
    state, reward, done = env.step(action)
    env.render()

                        
#Explanation :
   State Representation:

State: [x, y, velocity_x, velocity_y] describes the UAV's position and velocity.
The position (x, y) is updated based on the velocity (vx, vy) and time step.
Actions:

Four possible actions:
Accelerate left (vx -= 0.1).
Accelerate right (vx += 0.1).
Accelerate upward (vy += 0.1).
Accelerate downward (vy -= 0.1).
Obstacles and Bounds:

Static obstacles are defined as points in the environment (self.obstacles).
Environment bounds prevent the UAV from moving outside a predefined area.
Reward Function:

Goal Reached: Large positive reward for successfully reaching the target.
Collision: Large negative penalty for hitting an obstacle.
Step Penalty: Small penalty for every time step to encourage efficiency.
Episode End Condition:

The episode ends if the UAV collides with an obstacle or reaches the goal.



Step 4:Agent Architecture Design




                        
